{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process using Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a simple roll dice game will be modeled as a MDP problem.  The framework of mdptoolbox will be used to \n",
    "solve the game and Value Iteration used to find the optimal policy and expected value of the initial state given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "#### States\n",
    "The states will represent the earnings of the agent.  Thus, the agent starts at state 0 representing 0 initial earnings.\n",
    "The agent can then transition to other states or earnings by rolling the die and gaining rewards.\n",
    "\n",
    "#### Actions\n",
    "An action is required to move the agent from one state to the next.  This is modeled by a transition matrix of all of\n",
    "the probabilities from moving from one state to the next given an action.  A 3D matrix of the first set of indices being \n",
    "the type of action, the second set of indices or the rows being the initial state from whence the action is being taken \n",
    "from, and the third set of indices or the columns being the transition state to where the state is being transitioned to.\n",
    "The values given by the two sets of indices give probability of that transition happening.\n",
    "\n",
    "#### Rewards\n",
    "The rewards is the immediate benefit from taking an action in a state.  In this case it is rolling the die and rolling a\n",
    "valid number.  The reward is equal to the face value of the die.\n",
    "\n",
    "### Bellman Equation\n",
    "The Markov Decision Process is solved by the Bellman Equation, which calculates the long-term value of a state and \n",
    "the optimal policy of a state, meaning which action should the agent take in each state.  \n",
    "\n",
    "The long term value of a state is given by taking the action which maximizes the short term reward as well as the sum \n",
    "of the value all of the possible transition states taking into account a discount factor and the probability of \n",
    "transitioning into those future states.  More formally: \n",
    "\n",
    "Q(s) = arg.max( R(s,ai) +  $\\gamma$ * $\\sum_{j}$T(s, ai, sj) * Q(sj))\n",
    "\n",
    "\n",
    "Q(s)  = The quality of a state, which is the long-term value of being in a state\n",
    "\n",
    "ai = each action that could be taken from state s\n",
    "\n",
    "R(s, ai) = the reward for taking action ai being in state s\n",
    "\n",
    "T(s, ai, sj) = the transition matrix, which gives the probabilities for transistioning from state s to sj taking action ai\n",
    "\n",
    "Q(sj) = the quality of state sj\n",
    "\n",
    "$\\gamma$ = the discount factor, < 1 and models the cost of transitioning from one state to the next.  If this was >=1,\n",
    "the model would never converge and each state would have infinite benefit.\n",
    "\n",
    "### Value Iteration\n",
    "Value iteration is the process of continuously updating the optimal policy and values of a MDP model \n",
    "by recursively solving the Bellman Equation until the differences between successive runs reaches some epsilon.  In the mdptoolbox the default is epsilon = .01\n",
    "\n",
    "A detailed breakdown of the method is found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Simple Coin Toss Example\n",
    "\n",
    "In a simple coin toss game, you can choose to flip the coin or leave the game and take your earnings.  If you decide to\n",
    " flip you gain a reward of 1 if heads or if tails you will lose all of your earnings.\n",
    "\n",
    "Below illustrates the state and transitions for this markov decision process.  p is the probability of the transition,\n",
    "r is the reward for the transition, and a is the type of action to take.  a=1 is flipping the coin, a=0 is leaving the game.\n",
    "\n",
    "There is a 100% chance that you will remain in the same state if you leave the game.  If you flip a coin then you will get \n",
    "a reward of 1.  If you flip and loss, you will lose what you have accumulated in the game and transition to the terminal state.\n",
    "\n",
    "Not shown is the transition after you chose to leave the game and remain in the same state, you will transition to the terminal state.\n",
    "\n",
    "![markov example](./_markov_coin_flip.png)\n",
    "### Initialization\n",
    "We will use the mdptoolbox which is a python package for solving MDP problems as well as numpy which is a library which provides abstractions for matrix and array manipulations.\n",
    "\n",
    "The actions are to flip the coin to leave the game.  The states represent the earnings you have.  The reward represents the earnings gained by flipping the coin and landing on the desired side.\n",
    "\n",
    "The discount rate will be close to 1 or .99999 for the sake of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "# the number of sides of the dice/coin\n",
    "n_sides = 2\n",
    "\n",
    "# the number of runs to simulate\n",
    "n_runs = 2\n",
    "\n",
    "# the number of actions\n",
    "n_actions = 2\n",
    "\n",
    "# beginning and ending states\n",
    "n_initial_terminal_states = 2\n",
    "\n",
    "# the number of total states: the - n_runs is due to the fact that two of the states\n",
    "# end up in the terminal state so we can subtract them from the number of overall states\n",
    "n_states = n_runs * n_sides + n_initial_terminal_states  - n_sides\n",
    "\n",
    "# the boolean mask to indicate which states you will loose money on\n",
    "isBadSide = np.array([0]*n_states)\n",
    "\n",
    "isGoodSide = [not i for i in isBadSide]\n",
    "\n",
    "# the array which contains the values of the die\n",
    "die = np.array([1] * n_states)\n",
    "\n",
    "# the total earnings given a die roll\n",
    "earnings = die * isGoodSide  # [1, 1]\n",
    "\n",
    "# Calculate probability for Input:\n",
    "probability_dice = 1.0 / n_sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transitions\n",
    "The transition matrix represents all of the possible transitions between all of the states.\n",
    "\n",
    "There are two actions, thus the transition matrix will be of size 2.  \n",
    "\n",
    "For each, action there will be a probability of transitioning between each state.  Thus the transition matrix will be \n",
    "of n_actions * n_states * n_states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "transition_matrix = np.zeros([n_actions, n_states, n_states])\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action: Leave the Game \n",
    "Probabilities of the transitions of leaving the game and keeping the same score.  There is 100% chance that you can remain\n",
    "in the same state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in range(len(transition_matrix[0])):\n",
    "    transition_matrix[0][i][i] = 1\n",
    "            \n",
    "print(transition_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action: Roll the Dice\n",
    "The probabilities of transitions of rolling the die.  From the first state there is a .5 chance of going to state 1 and \n",
    ".5 chance of getting tails and going to the terminal state.  The third and fourth state are terminal states with no chance\n",
    "of the agent moving from from any other state than the ending state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.  0.5 0.  0.5]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.  1. ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in range(len(transition_matrix[1]) -2):\n",
    "    transition_matrix[1][i][i+1] = .5\n",
    "    transition_matrix[1][i][-1] = .5\n",
    "\n",
    "for i in range(len(transition_matrix[1])-2, len(transition_matrix[1])):\n",
    "    transition_matrix[1][i][-1] = 1\n",
    "\n",
    "print(transition_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reward: Initialization and Leaving the Game\n",
    "The reward matrix is the same dimension as the transition matrix.\n",
    "There is no reward for leaving the game.  Rewards are only accumulated by rolling the die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.zeros([n_actions, n_states, n_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reward: Rolling the Die\n",
    "Rolling the die can result in gaining a reward if it was heads or losing everything.  The ending column describes\n",
    "the result of rolling a tails and losing the accumulated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  2. -1.]\n",
      " [ 0.  0.  0. -2.]\n",
      " [ 0.  0.  0. -3.]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "reward_acc = 1\n",
    "for i in range(len(transition_matrix[1])-1):\n",
    "    reward_matrix[1][i][i+1] = reward_acc\n",
    "    reward_matrix[1][i][-1] = (1- reward_acc)\n",
    "    reward_acc+=1\n",
    "    \n",
    "reward_matrix[1][len(transition_matrix[1])-1][len(transition_matrix[1])-1] = (1 - reward_acc)\n",
    "\n",
    "print(reward_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initializing the MDP: Value Iteration Model\n",
    "Using the mdptoolbox the Value Iteration Model is initiated and run with the given transition and reward matricies.\n",
    "The library provides a high-level abstraction of the MDP, please see below for a more detailed breakdown of how the\n",
    "framework actually performs the value iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discount_factor = .99999  # less than 1 for convergence\n",
    "vi = mdptoolbox.mdp.ValueIteration(transition_matrix, reward_matrix, discount_factor)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The optimal policy indicates which action to take in each state.  The expected_values indicates what is the\n",
    "value of the state and how many we points we can expect by following the optimal policy.\n",
    "\n",
    "The optimal policy and expected value are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(1, 1, 0, 0)\n",
      "(0.7499975, 0.5, 0.0, 0.0)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "\n",
    "print optimal_policy\n",
    "print expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the optimal policy:  (1, 1, 0, 0) we can see that we should choose action 1 or flip the coin in the initial state\n",
    "and the second state or the first two flips.\n",
    "\n",
    "The value of each state is: (0.7499975, 0.5, 0.0, 0.0).  The value at the initial state is .75, .5 in the second state.\n",
    "\n",
    "## N-Die Roll Game\n",
    "The same game is played with the sides of the die at N=6.  The isBadSide = [1, 1, 1, 0 , 0, 0] or in other words\n",
    "the only rolls which will be rewarded is the complement or when a 4, 5, or 6 is rolled.\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the number of sides of the dice\n",
    "n_sides = 6\n",
    "\n",
    "# the number of runs to simulate\n",
    "n_runs = 2\n",
    "\n",
    "# the number of actions\n",
    "n_actions = 2\n",
    "\n",
    "# beginning and ending states\n",
    "n_initial_terminal_states = 2\n",
    "\n",
    "# the number of total states\n",
    "n_states = n_runs * n_sides + n_initial_terminal_states  # from 0 to 2N, plus quit\n",
    "\n",
    "# the boolean mask to indicate which states you will loose money on\n",
    "isBadSide = np.array([1, 1, 1, 0, 0, 0])\n",
    "isGoodSide = [not i for i in isBadSide]\n",
    "\n",
    "# the array which contains the values of the die\n",
    "die = np.arange(1, n_sides + 1)  # [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# the total earnings given a die roll\n",
    "earnings = die * isGoodSide  # [0, 0, 0, 4, 5, 6]\n",
    "\n",
    "# Calculate probability for Input:\n",
    "probability_dice = 1.0 / n_sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action: Do not Roll the Dice\n",
    "\n",
    "There 100% chance that you do not have to roll the dice if you are in a given state and you will not transition to\n",
    "another state but will remain in the same one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "transition_matrix = np.zeros([n_actions, n_states, n_states])\n",
    "# the probability matrix for the first action, if you do not roll\n",
    "for i in range(len(transition_matrix[0])):\n",
    "    transition_matrix[0][i][i] = 1\n",
    "            \n",
    "print(transition_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: Roll the Dice\n",
    "The first row gives the probabilities of transitioning from the initial state to the possible states of valid dice\n",
    "rolls and the state of rolling a game ending dice roll which is represented by the last column.  In this example, there \n",
    "is a p chance of rolling a 4, 5, 6, and a 1/2 chance of transitioning to the final state which is losing,\n",
    "all of the money earned thus far (which is 0 at state 0).\n",
    "\n",
    "The second row gives all of the possible transitions given earnings of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.16666667\n",
      "  0.16666667 0.16666667 0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.16666667 0.16666667 0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667 0.16666667 0.16666667 0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.16666667 0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.16666667\n",
      "  0.16666667 0.66666667]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.83333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]]\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "#if roll\n",
    "p=1.0/n_sides\n",
    "min_roll = 4\n",
    "n_min_states = 3\n",
    "# after the first roll, you have a 1/6 chance of transistioning \n",
    "for i in range(n_states -1):\n",
    "    for j in range(min_roll, min(min_roll + n_min_states, n_states- i)):\n",
    "        transition_matrix[1][i][i+j] = p\n",
    "        transition_matrix[1][i][-1] = 1 - sum(transition_matrix[1][i][:-1])\n",
    "\n",
    "for i in range(len(transition_matrix[1])-min_roll, len(transition_matrix[1])):\n",
    "    transition_matrix[1][i][-1] = 1\n",
    "\n",
    "print(transition_matrix[1])\n",
    "\n",
    "np.sum(transition_matrix[0],axis=1)\n",
    "np.sum(transition_matrix[1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Transition Matrix\n",
    "The first state with index 0 is the initial state before any roll of the die. The last state\n",
    "or index 13 is the terminal state.  All the other states represent the amount of earnings accumulated.\n",
    "![transition matrix](./markov.png)\n",
    "\n",
    "### Reward: Initialization and Leaving the Game\n",
    "The reward matrix is the same dimension as the transition matrix.\n",
    "There is no reward for leaving the game.  Rewards are only accumulated by rolling the die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.zeros([n_actions, n_states, n_states])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward: Rolling the Die\n",
    "Rolling the die can result in gaining a reward if it was heads or losing everything.  \n",
    "The ending column describes the result of rolling a tails and losing the accumulated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[  0.   0.   0.   0.   4.   5.   6.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   4.   5.   6.   0.   0.   0.   0.   0.  -1.]\n",
      " [  0.   0.   0.   0.   0.   0.   4.   5.   6.   0.   0.   0.   0.  -2.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   4.   5.   6.   0.   0.   0.  -3.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   4.   5.   6.   0.   0.  -4.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   5.   6.   0.  -5.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   5.   6.  -6.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   5.  -7.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  -8.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -9.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -11.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -12.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -13.]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#if roll\n",
    "reward_acc = 4\n",
    "reward_curr = 4\n",
    "reward_tot = 0\n",
    "n_min_states = 3\n",
    "\n",
    "for i in range(n_states-1):\n",
    "    reward_curr = reward_acc\n",
    "    for j in range(reward_curr, min(reward_curr + n_min_states, n_states - i)):\n",
    "        reward_matrix[1][i][i + j] = reward_curr\n",
    "        reward_curr +=1\n",
    "    \n",
    "    reward_matrix[1][i][-1] = -reward_tot\n",
    "    if i ==0:\n",
    "        reward_tot+=1\n",
    "    else:\n",
    "        reward_tot+=1\n",
    "\n",
    "reward_matrix[1][n_states -1][n_states -1] = -reward_tot \n",
    "\n",
    "print(reward_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results: Reward Matrix\n",
    "\n",
    "![reward_matrix](./reward_matrix.png)\n",
    "\n",
    "### Initializing the MDP: Value Iteration Model\n",
    "Using the mdptoolbox the Value Iteration Model is initiated and run with the given transition and reward matricies.\n",
    "The library provides a high-level abstraction of the MDP, please see below for a more detailed breakdown of how the\n",
    "framework actually performs the value iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "(2.583325, 2.0, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "discount_factor = .9999\n",
    "\n",
    "vi = mdptoolbox.mdp.ValueIteration(transition_matrix, reward_matrix, discount_factor)\n",
    "vi.run()\n",
    "\n",
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "\n",
    "print optimal_policy\n",
    "print expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results: N-Die Roll\n",
    "Running the simulation, the optimal policy is to roll the die if you have earnings below or equal to $4.\n",
    "\n",
    "With 0 initial earnings you are expected to make $2.58 using the optimal policy.\n",
    "\n",
    "### Markov Decision Process: Toolbox: Detailed Breakdown\n",
    "The following gives more information on how the value iteration actually works within the MDP: Toolbox library.\n",
    "\n",
    "### MDP Class\n",
    "The MDP class has a number of important parameters and attributes.  The following is the\n",
    "documentation of the MDP class.\n",
    "\n",
    "    A Markov Decision Problem.\n",
    "\n",
    "    Let ``S`` = the number of states, and ``A`` = the number of acions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions : array\n",
    "        Transition probability matrices. These can be defined in a variety of\n",
    "        ways. The simplest is a numpy array that has the shape ``(A, S, S)``,\n",
    "        though there are other possibilities. It can be a tuple or list or\n",
    "        numpy object array of length ``A``, where each element contains a numpy\n",
    "        array or matrix that has the shape ``(S, S)``. This \"list of matrices\"\n",
    "        form is useful when the transition matrices are sparse as\n",
    "        ``scipy.sparse.csr_matrix`` matrices can be used. In summary, each\n",
    "        action's transition matrix must be indexable like ``transitions[a]``\n",
    "        where ``a`` ∈ {0, 1...A-1}, and ``transitions[a]`` returns an ``S`` ×\n",
    "        ``S`` array-like object.\n",
    "    reward : array\n",
    "        Reward matrices or vectors. Like the transition matrices, these can\n",
    "        also be defined in a variety of ways. Again the simplest is a numpy\n",
    "        array that has the shape ``(S, A)``, ``(S,)`` or ``(A, S, S)``. A list\n",
    "        of lists can be used, where each inner list has length ``S`` and the\n",
    "        outer list has length ``A``. A list of numpy arrays is possible where\n",
    "        each inner array can be of the shape ``(S,)``, ``(S, 1)``, ``(1, S)``\n",
    "        or ``(S, S)``. Also ``scipy.sparse.csr_matrix`` can be used instead of\n",
    "        numpy arrays. In addition, the outer list can be replaced by any object\n",
    "        that can be indexed like ``reward[a]`` such as a tuple or numpy object\n",
    "        array of length ``A``.\n",
    "    discount : float\n",
    "        Discount factor. The per time-step discount factor on future rewards.\n",
    "        Valid values are greater than 0 upto and including 1. If the discount\n",
    "        factor is 1, then convergence is cannot be assumed and a warning will\n",
    "        be displayed. Subclasses of ``MDP`` may pass ``None`` in the case where\n",
    "        the algorithm does not use a discount factor.\n",
    "    epsilon : float\n",
    "        Stopping criterion. The maximum change in the value function at each\n",
    "        iteration is compared against ``epsilon``. Once the change falls below\n",
    "        this value, then the value function is considered to have converged to\n",
    "        the optimal value function. Subclasses of ``MDP`` may pass ``None`` in\n",
    "        the case where the algorithm does not use an epsilon-optimal stopping\n",
    "        criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations. The algorithm will be terminated once\n",
    "        this many iterations have elapsed. This must be greater than 0 if\n",
    "        specified. Subclasses of ``MDP`` may pass ``None`` in the case where\n",
    "        the algorithm does not use a maximum number of iterations.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    P : array\n",
    "        Transition probability matrices.\n",
    "    R : array\n",
    "        Reward vectors.\n",
    "    V : tuple\n",
    "        The optimal value function. Each element is a float corresponding to\n",
    "        the expected value of being in that state assuming the optimal policy\n",
    "        is followed.\n",
    "    discount : float\n",
    "        The discount rate on future rewards.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations.\n",
    "    policy : tuple\n",
    "        The optimal policy.\n",
    "    time : float\n",
    "        The time used to converge to the optimal policy.\n",
    "    verbose : boolean\n",
    "        Whether verbose output should be displayed or not.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run\n",
    "        Implemented in child classes as the main algorithm loop. Raises an\n",
    "        exception if it has not been overridden.\n",
    "\n",
    "### MDP: Rewards Matrix\n",
    "The reward matrix is calculating by multiplying the probability transition matrix by the corresponding reward value.  \n",
    "It is then summed across the entire state to get to overall expected reward from the state.  For example, \n",
    "if we have a 1/6 chance of getting 4, 1/6 chance of rolling a 5, and 1/6 chance of rolling a 6 and 1/2 chance of 0.  \n",
    "Then the expected value is Transitions * Reward Matrix = Expected Reward Matrix = 2.5.  This will be used\n",
    "as the reward in the Bellman Equation.\n",
    "\n",
    "The fifth state of index 4 which represents an agent earnings of 4 happens if you were to roll a 4 on your first roll.\n",
    "From this state, you can leave in which case you would stay at 4 and get 0 reward.  Or you can roll the die, in which case you\n",
    "would have 1/2 chance of rolling a bad side and losing your earnings of -4.  1/6 chance of getting 8, 1/6 or 9, 1/6 or 10.\n",
    "\n",
    "See the results matrix above for a visualization of this matrix.\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "    def _computeMatrixReward(self, reward, transition):\n",
    "    \n",
    "        if _sp.issparse(reward):\n",
    "            return reward.multiply(transition).sum(1).A.reshape(self.S)\n",
    "        elif  _sp.issparse(transition):\n",
    "            return transition.multiply(reward).sum(1).A.reshape(self.S)\n",
    "        else:\n",
    "            tran_reward = _np.multiply(transition, reward)\n",
    "            sum_tran_reward = tran_reward.sum(1)\n",
    "            ret_val = sum_tran_reward.reshape(self.S)\n",
    "            return ret_val\n",
    "\n",
    "\n",
    "### MDP: Value Iteration Class\n",
    "\n",
    "Value Iteration is a subclass of the MDP super class.  The run method actually executes the algorithm.\n",
    "\n",
    "    A discounted MDP solved using the value iteration algorithm.\n",
    "\n",
    "    Description\n",
    "    -----------\n",
    "    ValueIteration applies the value iteration algorithm to solve a\n",
    "    discounted MDP. The algorithm consists of solving Bellman's equation\n",
    "    iteratively.\n",
    "    Iteration is stopped when an epsilon-optimal policy is found or after a\n",
    "    specified number (``max_iter``) of iterations.\n",
    "    This function uses verbose and silent modes. In verbose mode, the function\n",
    "    displays the variation of ``V`` (the value function) for each iteration and\n",
    "    the condition which stopped the iteration: epsilon-policy found or maximum\n",
    "    number of iterations reached.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions : array\n",
    "        Transition probability matrices. See the documentation for the ``MDP``\n",
    "        class for details.\n",
    "    reward : array\n",
    "        Reward matrices or vectors. See the documentation for the ``MDP`` class\n",
    "        for details.\n",
    "    discount : float\n",
    "        Discount factor. See the documentation for the ``MDP`` class for\n",
    "        details.\n",
    "    epsilon : float, optional\n",
    "        Stopping criterion. See the documentation for the ``MDP`` class for\n",
    "        details.  Default: 0.01.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations. If the value given is greater than a\n",
    "        computed bound, a warning informs that the computed bound will be used\n",
    "        instead. By default, if ``discount`` is not equal to 1, a bound for\n",
    "        ``max_iter`` is computed, otherwise ``max_iter`` = 1000. See the\n",
    "        documentation for the ``MDP`` class for further details.\n",
    "    initial_value : array, optional\n",
    "        The starting value function. Default: a vector of zeros.\n",
    "\n",
    "    Data Attributes\n",
    "    ---------------\n",
    "    V : tuple\n",
    "        The optimal value function.\n",
    "    policy : tuple\n",
    "        The optimal policy function. Each element is an integer corresponding\n",
    "        to an action which maximises the value function in that state.\n",
    "    iter : int\n",
    "        The number of iterations taken to complete the computation.\n",
    "    time : float\n",
    "        The amount of CPU time used to run the algorithm.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run()\n",
    "        Do the algorithm iteration.\n",
    "   \n",
    "### MDP: Value Iteration Method\n",
    "\n",
    "Once initialized, the run method starts the Value Iteration algorithm and calculates the optimal values and \n",
    "optimal policy at each state using the Bellman operator until convergence which is measured by epsilon.\n",
    "\n",
    "At each iteration, the optimal values V (which belong to the class) are updated by calling the \n",
    "_bellmanOperator() function.  The difference between the new value and the old value is then taken and\n",
    "compared to epsilon, the stopping condition.  \n",
    "\n",
    "        def run(self):\n",
    "            # Run the value iteration algorithm.\n",
    "    \n",
    "            if self.verbose:\n",
    "                print('  Iteration\\t\\tV-variation')\n",
    "    \n",
    "            self.time = _time.time()\n",
    "            while True:\n",
    "                self.iter += 1\n",
    "    \n",
    "                Vprev = self.V.copy()\n",
    "    \n",
    "                # Bellman Operator: compute policy and value functions\n",
    "                self.policy, self.V = self._bellmanOperator()\n",
    "    \n",
    "                # The values, based on Q. For the function \"max()\": the option\n",
    "                # \"axis\" means the axis along which to operate. In this case it\n",
    "                # finds the maximum of the the rows. (Operates along the columns?)\n",
    "                variation = _util.getSpan(self.V - Vprev)\n",
    "    \n",
    "                if self.verbose:\n",
    "                    print((\"    %s\\t\\t  %s\" % (self.iter, variation)))\n",
    "    \n",
    "                if variation < self.thresh:\n",
    "                    if self.verbose:\n",
    "                        print(_MSG_STOP_EPSILON_OPTIMAL_POLICY)\n",
    "                    break\n",
    "                elif self.iter == self.max_iter:\n",
    "                    if self.verbose:\n",
    "                        print(_MSG_STOP_MAX_ITER)\n",
    "                    break\n",
    "    \n",
    "            # store value and policy as tuples\n",
    "            self.V = tuple(self.V.tolist())\n",
    "            self.policy = tuple(self.policy.tolist())\n",
    "    \n",
    "            self.time = _time.time() - self.time\n",
    "\n",
    "### MDP: Bellman Operator Method\n",
    "The initial values are 0 for each state.  \n",
    "\n",
    "The first action to leave the game, gets 0 reward and there is a 100% probability that the current value is given.\n",
    "Thus the old value is equal to the new value.\n",
    "\n",
    "The second action to roll the dice, the Bellman equation is more interesting.\n",
    "\n",
    "Q[aa] = self.R[aa] + self.discount * self.P[aa].dot(V)\n",
    "\n",
    "First, the reward for each state was precomputed in the Reward Matrix Above.  The probability of the transition is\n",
    "has also been given as a parameter.  The current value is initially 0.\n",
    "Thus the Quality of the state given rolling the die is just the Reward value at that state.  Thus the intial state, the\n",
    "reward is then 2.5.\n",
    "\n",
    "The second iteration, the input values are now the rewards matrix.  The rewards matrix does not change. The dot product\n",
    "between the input values and the probability of those transitions gives the reward of future transitions.  The only\n",
    "non-zero term is the transition from the initial state to the 5th state or a roll of 4.  The probability of the transition \n",
    "to roll 4 is 1/6 and the reward per the overall reward per the reward matrix is .5.  Thus the added value is then 1/6 * .5 = \n",
    ".08333.  Thus the new value is 2.8333.  This value does not change further as the value of the 5th state has already been\n",
    "computed and factored into the vallue update to the initial value.\n",
    " \n",
    "The bellman operator method:\n",
    "\n",
    "        def _bellmanOperator(self, V=None):\n",
    "            # Apply the Bellman operator on the value function.\n",
    "            #\n",
    "            # Updates the value function and the Vprev-improving policy.\n",
    "            #\n",
    "            # Returns: (policy, value), tuple of new policy and its value\n",
    "            #\n",
    "            # If V hasn't been sent into the method, then we assume to be working\n",
    "            # on the objects V attribute\n",
    "            if V is None:\n",
    "                # this V should be a reference to the data rather than a copy\n",
    "                V = self.V\n",
    "            else:\n",
    "                # make sure the user supplied V is of the right shape\n",
    "                try:\n",
    "                    assert V.shape in ((self.S,), (1, self.S)), \"V is not the \" \\\n",
    "                        \"right shape (Bellman operator).\"\n",
    "                except AttributeError:\n",
    "                    raise TypeError(\"V must be a numpy array or matrix.\")\n",
    "            # Looping through each action the the Q-value matrix is calculated.\n",
    "            # P and V can be any object that supports indexing, so it is important\n",
    "            # that you know they define a valid MDP before calling the\n",
    "            # _bellmanOperator method. Otherwise the results will be meaningless.\n",
    "            Q = _np.empty((self.A, self.S))\n",
    "            for aa in range(self.A):\n",
    "                Q[aa] = self.R[aa] + self.discount * self.P[aa].dot(V)\n",
    "    \n",
    "            # Get the policy and value, for now it is being returned but...\n",
    "            # Which way is better?\n",
    "            # 1. Return, (policy, value)\n",
    "    \n",
    "            Qmax = (Q.argmax(axis=0), Q.max(axis=0))\n",
    "            return Qmax\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
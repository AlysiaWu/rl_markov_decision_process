{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process using Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a simple roll dice game will be modeled as a MDP problem.  The framework of mdptoolbox will be used to \n",
    "solve the game and Value Iteration used to find the optimal policy and expected value of the initial state given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "#### States\n",
    "The states will represent the earnings of the agent.  Thus, the agent starts at state 0 representing 0 initial earnings.\n",
    "The agent can then transition to other states or earnings by rolling the die and gaining rewards.\n",
    "\n",
    "#### Actions\n",
    "An action is required to move the agent from one state to the next.  This is modeled by a transition matrix of all of\n",
    "the probabilities from moving from one state to the next given an action.  A 3D matrix of the first set of indices being \n",
    "the type of action, the second set of indices or the rows being the initial state from whence the action is being taken \n",
    "from, and the third set of indices or the columns being the transition state to where the state is being transitioned to.\n",
    "The values given by the two sets of indices give probability of that transition happening.\n",
    "\n",
    "#### Rewards\n",
    "The rewards is the immediate benefit from taking an action in a state.  In this case it is rolling the die and rolling a\n",
    "valid number.  The reward is equal to the face value of the die.\n",
    "\n",
    "### Bellman Equation\n",
    "The Markov Decision Process is solved by the Bellman Equation, which is given by:\n",
    "\n",
    "Q(s) = arg.max( R(s,ai) +  $\\sum_{j}$T(s, ai, sj) * Q(sj))\n",
    "\n",
    "ai = each action that could be taken from state s\n",
    "Q(s)  = The quality of a state, which is the long-term value of being in a state\n",
    "\n",
    "\n",
    "### Value Iteration\n",
    "Given the states, actions, and rewards a MDP model can be constructed and solved by value iteration.  Value iteration is the\n",
    "process of continuously updating the optimal policy and values of a MDP model by solving the Bellman Equation and updating the\n",
    "optimal values in a recursive way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Simple Coin Toss Example\n",
    "\n",
    "In a simple coin toss game, you can choose to flip the coin or leave the game and take your earnings.  If you decide to\n",
    " flip you gain a reward of 1 if heads or if tails you will lose all of your earnings.\n",
    "\n",
    "Below illustrates the state and transitions for this markov decision process.\n",
    "\n",
    "![markov example](./markov.png)\n",
    "### Initialization\n",
    "We will use the mdptoolbox which is a python package for solving MDP problems as well as numpy which is a library which\n",
    " provides abstractions for matrix and array manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "# the number of sides of the dice/coin\n",
    "n_sides = 2\n",
    "\n",
    "# the number of runs to simulate\n",
    "n_runs = 2\n",
    "\n",
    "# the number of actions\n",
    "n_actions = 2\n",
    "\n",
    "# beginning and ending states\n",
    "n_initial_terminal_states = 2\n",
    "\n",
    "# the number of total states: the - n_runs is due to the fact that two of the states\n",
    "# end up in the terminal state so we can subtract them from the number of overall states\n",
    "n_states = n_runs * n_sides + n_initial_terminal_states  - n_sides\n",
    "\n",
    "# the boolean mask to indicate which states you will loose money on\n",
    "isBadSide = np.array([0]*n_states)\n",
    "\n",
    "isGoodSide = [not i for i in isBadSide]\n",
    "\n",
    "# the array which contains the values of the die\n",
    "die = np.array([1] * n_states)\n",
    "\n",
    "# the total earnings given a die roll\n",
    "earnings = die * isGoodSide  # [1, 1]\n",
    "\n",
    "# Calculate probability for Input:\n",
    "probability_dice = 1.0 / n_sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transitions\n",
    "The transition matrix represents all of the possible transitions between all of the states.\n",
    "\n",
    "There are two actions, thus the transition matrix will be of size 2.  \n",
    "\n",
    "For each, action there will be a probability of transitioning between each state.  Thus the transition matrix will be \n",
    "of n_actions * n_states * n_states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "transition_matrix = np.zeros([n_actions, n_states, n_states])\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action: Leave the Game \n",
    "Probabilities of the transitions of leaving the game and keeping the same score.  There is 100% chance that you can remain\n",
    "in the same state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(transition_matrix[0])):\n",
    "    transition_matrix[0][i][i] = 1\n",
    "            \n",
    "print(transition_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action: Roll the Dice\n",
    "The probabilities of transitions of rolling the die.  From the first state there is a .5 chance of going to state 1 and \n",
    ".5 chance of getting tails and going to the terminal state.  The third and fourth state are terminal states with no chance\n",
    "of the agent moving from from any other state than the ending state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5 0.  0.5]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(transition_matrix[1]) -2):\n",
    "    transition_matrix[1][i][i+1] = .5\n",
    "    transition_matrix[1][i][-1] = .5\n",
    "\n",
    "for i in range(len(transition_matrix[1])-2, len(transition_matrix[1])):\n",
    "    transition_matrix[1][i][-1] = 1\n",
    "\n",
    "print(transition_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reward: Initialization and Leaving the Game\n",
    "The reward matrix is the same dimension as the transition matrix.\n",
    "There is no reward for leaving the game.  Rewards are only accumulated by rolling the die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.zeros([n_actions, n_states, n_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reward: Rolling the Die\n",
    "Rolling the die can result in gaining a reward if it was heads or losing everything.  The ending column describes\n",
    "the result of rolling a tails and losing the accumulated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  2. -1.]\n",
      " [ 0.  0.  0. -2.]\n",
      " [ 0.  0.  0. -3.]]\n"
     ]
    }
   ],
   "source": [
    "reward_acc = 1\n",
    "for i in range(len(transition_matrix[1])-1):\n",
    "    reward_matrix[1][i][i+1] = reward_acc\n",
    "    reward_matrix[1][i][-1] = (1- reward_acc)\n",
    "    reward_acc+=1\n",
    "    \n",
    "reward_matrix[1][len(transition_matrix[1])-1][len(transition_matrix[1])-1] = (1 - reward_acc)\n",
    "\n",
    "print(reward_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initializing the MDP: Value Iteration Model\n",
    "Using the mdptoolbox the Value Iteration Model is initiated and run with the given transition and reward matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discount_factor = .99999  # less than 1 for convergence\n",
    "vi = mdptoolbox.mdp.ValueIteration(transition_matrix, reward_matrix, discount_factor)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The optimal policy indicates which action to take in each state.  The expected_values indicates what is the\n",
    "value of the state and how many we points we can expect by following the optimal policy.\n",
    "                \n",
    "The optimal policy and expected value are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 0, 0)\n",
      "(0.7499975, 0.5, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "\n",
    "print optimal_policy\n",
    "print expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the optimal policy:  (1, 1, 0, 0) we can see that we should choose action 1 or flip the coin in the initial state\n",
    "and the second state or the first two flips.\n",
    "\n",
    "The value of each state is: (0.7499975, 0.5, 0.0, 0.0).  The value at the initial state is .75, .5 in the second state.\n",
    "\n",
    "## N-Die Roll Game\n",
    "The same game is played with the sides of the die at N=6.  The isBadSide = [1, 1, 1, 0 , 0, 0] or in other words\n",
    "the only rolls which will be rewarded is the complement or when a 4, 5, or 6 is rolled.\n",
    "\n",
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the number of sides of the dice\n",
    "n_sides = 6\n",
    "\n",
    "# the number of runs to simulate\n",
    "n_runs = 2\n",
    "\n",
    "# the number of actions\n",
    "n_actions = 2\n",
    "\n",
    "# beginning and ending states\n",
    "n_initial_terminal_states = 2\n",
    "\n",
    "# the number of total states\n",
    "n_states = n_runs * n_sides + n_initial_terminal_states  # from 0 to 2N, plus quit\n",
    "\n",
    "# the boolean mask to indicate which states you will loose money on\n",
    "isBadSide = np.array([1, 1, 1, 0, 0, 0])\n",
    "isGoodSide = [not i for i in isBadSide]\n",
    "\n",
    "# the array which contains the values of the die\n",
    "die = np.arange(1, n_sides + 1)  # [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# the total earnings given a die roll\n",
    "earnings = die * isGoodSide  # [0, 0, 0, 4, 5, 6]\n",
    "\n",
    "# Calculate probability for Input:\n",
    "probability_dice = 1.0 / n_sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Action: Do not Roll the Dice\n",
    "\n",
    "There 100% chance that you do not have to roll the dice if you are in a given state and you will not transition to\n",
    "another state but will remain in the same one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "transition_matrix = np.zeros([n_actions, n_states, n_states])\n",
    "# the probability matrix for the first action, if you do not roll\n",
    "for i in range(len(transition_matrix[0])):\n",
    "    transition_matrix[0][i][i] = 1\n",
    "            \n",
    "print(transition_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action: Roll the Dice\n",
    "The first row gives the probabilities of transitioning from the initial state to the possible states of valid dice\n",
    "rolls and the state of rolling a game ending dice roll which is represented by the last column.  In this example, there \n",
    "is a p chance of rolling a 4, 5, 6, and a 1/2 chance of transitioning to the final state which is losing,\n",
    "all of the money earned thus far (which is 0 at state 0).\n",
    "\n",
    "The second row gives all of the possible transitions from the second state.  We can see that the the same ratio exists\n",
    "from transitioning, but is shifted down since the minimum valid state is given by rolling two 4's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.16666667\n",
      "  0.16666667 0.16666667 0.         0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.16666667 0.16666667 0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16666667 0.16666667 0.16666667 0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.16666667 0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.16666667 0.16666667\n",
      "  0.16666667 0.5       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.16666667\n",
      "  0.16666667 0.66666667]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16666667 0.83333333]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if roll\n",
    "p=1.0/n_sides\n",
    "min_roll = 4\n",
    "n_min_states = 3\n",
    "# after the first roll, you have a 1/6 chance of transistioning \n",
    "for i in range(n_states -1):\n",
    "    for j in range(min_roll, min(min_roll + n_min_states, n_states- i)):\n",
    "        transition_matrix[1][i][i+j] = p\n",
    "        transition_matrix[1][i][-1] = 1 - sum(transition_matrix[1][i][:-1])\n",
    "\n",
    "for i in range(len(transition_matrix[1])-min_roll, len(transition_matrix[1])):\n",
    "    transition_matrix[1][i][-1] = 1\n",
    "\n",
    "print(transition_matrix[1])\n",
    "\n",
    "np.sum(transition_matrix[0],axis=1)\n",
    "np.sum(transition_matrix[1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Transition Matrix\n",
    "![transition matrix](./markov.png)\n",
    "\n",
    "### Reward: Initialization and Leaving the Game\n",
    "The reward matrix is the same dimension as the transition matrix.\n",
    "There is no reward for leaving the game.  Rewards are only accumulated by rolling the die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reward_matrix = np.zeros([n_actions, n_states, n_states])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward: Rolling the Die\n",
    "Rolling the die can result in gaining a reward if it was heads or losing everything.  The ending column describes\n",
    "the result of rolling a tails and losing the accumulated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   4.   5.   6.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   4.   5.   6.   0.   0.   0.   0.   0.  -1.]\n",
      " [  0.   0.   0.   0.   0.   0.   4.   5.   6.   0.   0.   0.   0.  -2.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   4.   5.   6.   0.   0.   0.  -3.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   4.   5.   6.   0.   0.  -4.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   5.   6.   0.  -5.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   5.   6.  -6.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   5.  -7.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  -8.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -9.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -11.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -12.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -13.]]\n"
     ]
    }
   ],
   "source": [
    "#if roll\n",
    "reward_acc = 4\n",
    "reward_curr = 4\n",
    "reward_tot = 0\n",
    "n_min_states = 3\n",
    "\n",
    "for i in range(n_states-1):\n",
    "    reward_curr = reward_acc\n",
    "    for j in range(reward_curr, min(reward_curr + n_min_states, n_states - i)):\n",
    "        reward_matrix[1][i][i + j] = reward_curr\n",
    "        reward_curr +=1\n",
    "    \n",
    "    reward_matrix[1][i][-1] = -reward_tot\n",
    "    if i ==0:\n",
    "        reward_tot+=1\n",
    "    else:\n",
    "        reward_tot+=1\n",
    "\n",
    "reward_matrix[1][n_states -1][n_states -1] = -reward_tot \n",
    "\n",
    "print(reward_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results: Reward Matrix\n",
    "\n",
    "![reward_matrix](./reward_matrix.png)\n",
    "\n",
    "### Initializing the MDP: Value Iteration Model\n",
    "Using the mdptoolbox the Value Iteration Model is initiated and run with the given transition and reward matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "(2.583325, 2.0, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "discount_factor = .9999\n",
    "\n",
    "vi = mdptoolbox.mdp.ValueIteration(transition_matrix, reward_matrix, discount_factor)\n",
    "vi.run()\n",
    "\n",
    "optimal_policy = vi.policy\n",
    "expected_values = vi.V\n",
    "\n",
    "print optimal_policy\n",
    "print expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results: N-Die Roll\n",
    "Running the simulation, the optimal policy is to roll the die if you have earnings below or equal to $4.\n",
    "\n",
    "With 0 initial earnings you are expected to make $2.58 using the optimal policy.\n",
    "\n",
    "### Markov Decision Process: Toolbox\n",
    "The following gives more information on how the value iteration actually works within the MDP: Toolbox library.\n",
    "\n",
    "### MDP Class\n",
    "The MDP class has a number of important parameters and attributes.  The following is the\n",
    "documentation of the MDP class.\n",
    "\n",
    "    A Markov Decision Problem.\n",
    "\n",
    "    Let ``S`` = the number of states, and ``A`` = the number of acions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions : array\n",
    "        Transition probability matrices. These can be defined in a variety of\n",
    "        ways. The simplest is a numpy array that has the shape ``(A, S, S)``,\n",
    "        though there are other possibilities. It can be a tuple or list or\n",
    "        numpy object array of length ``A``, where each element contains a numpy\n",
    "        array or matrix that has the shape ``(S, S)``. This \"list of matrices\"\n",
    "        form is useful when the transition matrices are sparse as\n",
    "        ``scipy.sparse.csr_matrix`` matrices can be used. In summary, each\n",
    "        action's transition matrix must be indexable like ``transitions[a]``\n",
    "        where ``a`` ∈ {0, 1...A-1}, and ``transitions[a]`` returns an ``S`` ×\n",
    "        ``S`` array-like object.\n",
    "    reward : array\n",
    "        Reward matrices or vectors. Like the transition matrices, these can\n",
    "        also be defined in a variety of ways. Again the simplest is a numpy\n",
    "        array that has the shape ``(S, A)``, ``(S,)`` or ``(A, S, S)``. A list\n",
    "        of lists can be used, where each inner list has length ``S`` and the\n",
    "        outer list has length ``A``. A list of numpy arrays is possible where\n",
    "        each inner array can be of the shape ``(S,)``, ``(S, 1)``, ``(1, S)``\n",
    "        or ``(S, S)``. Also ``scipy.sparse.csr_matrix`` can be used instead of\n",
    "        numpy arrays. In addition, the outer list can be replaced by any object\n",
    "        that can be indexed like ``reward[a]`` such as a tuple or numpy object\n",
    "        array of length ``A``.\n",
    "    discount : float\n",
    "        Discount factor. The per time-step discount factor on future rewards.\n",
    "        Valid values are greater than 0 upto and including 1. If the discount\n",
    "        factor is 1, then convergence is cannot be assumed and a warning will\n",
    "        be displayed. Subclasses of ``MDP`` may pass ``None`` in the case where\n",
    "        the algorithm does not use a discount factor.\n",
    "    epsilon : float\n",
    "        Stopping criterion. The maximum change in the value function at each\n",
    "        iteration is compared against ``epsilon``. Once the change falls below\n",
    "        this value, then the value function is considered to have converged to\n",
    "        the optimal value function. Subclasses of ``MDP`` may pass ``None`` in\n",
    "        the case where the algorithm does not use an epsilon-optimal stopping\n",
    "        criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations. The algorithm will be terminated once\n",
    "        this many iterations have elapsed. This must be greater than 0 if\n",
    "        specified. Subclasses of ``MDP`` may pass ``None`` in the case where\n",
    "        the algorithm does not use a maximum number of iterations.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    P : array\n",
    "        Transition probability matrices.\n",
    "    R : array\n",
    "        Reward vectors.\n",
    "    V : tuple\n",
    "        The optimal value function. Each element is a float corresponding to\n",
    "        the expected value of being in that state assuming the optimal policy\n",
    "        is followed.\n",
    "    discount : float\n",
    "        The discount rate on future rewards.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations.\n",
    "    policy : tuple\n",
    "        The optimal policy.\n",
    "    time : float\n",
    "        The time used to converge to the optimal policy.\n",
    "    verbose : boolean\n",
    "        Whether verbose output should be displayed or not.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run\n",
    "        Implemented in child classes as the main algorithm loop. Raises an\n",
    "        exception if it has not been overridden.\n",
    "        \n",
    "### MDP: Constructor\n",
    "The constructor sets the parameters, the reward vector is calculating by multiplying the \n",
    "probability transition matrix by the corresponding reward value.  It is then summed across the entire\n",
    "state to get to overall expected reward from the state.  For example, if we have a 1/6 chance of getting 4,\n",
    "1/6 chance of rolling a 5, and 1/6 chance of rolling a 6 and 1/2 chance of 0.  Then the expected value is\n",
    "Transitions * Reward Matrix = Expected Reward Matrix = 2.5.\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "    def _computeMatrixReward(self, reward, transition):\n",
    "    \n",
    "        if _sp.issparse(reward):\n",
    "            return reward.multiply(transition).sum(1).A.reshape(self.S)\n",
    "        elif  _sp.issparse(transition):\n",
    "            return transition.multiply(reward).sum(1).A.reshape(self.S)\n",
    "        else:\n",
    "            tran_reward = _np.multiply(transition, reward)\n",
    "            sum_tran_reward = tran_reward.sum(1)\n",
    "            ret_val = sum_tran_reward.reshape(self.S)\n",
    "            return ret_val\n",
    "            \n",
    "### MDP: Value Iteration\n",
    "\n",
    "ValueIteration is a subclass of the MDP super class.  There is a wealth of information within the class:\n",
    "\n",
    "    A discounted MDP solved using the value iteration algorithm.\n",
    "\n",
    "    Description\n",
    "    -----------\n",
    "    ValueIteration applies the value iteration algorithm to solve a\n",
    "    discounted MDP. The algorithm consists of solving Bellman's equation\n",
    "    iteratively.\n",
    "    Iteration is stopped when an epsilon-optimal policy is found or after a\n",
    "    specified number (``max_iter``) of iterations.\n",
    "    This function uses verbose and silent modes. In verbose mode, the function\n",
    "    displays the variation of ``V`` (the value function) for each iteration and\n",
    "    the condition which stopped the iteration: epsilon-policy found or maximum\n",
    "    number of iterations reached.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions : array\n",
    "        Transition probability matrices. See the documentation for the ``MDP``\n",
    "        class for details.\n",
    "    reward : array\n",
    "        Reward matrices or vectors. See the documentation for the ``MDP`` class\n",
    "        for details.\n",
    "    discount : float\n",
    "        Discount factor. See the documentation for the ``MDP`` class for\n",
    "        details.\n",
    "    epsilon : float, optional\n",
    "        Stopping criterion. See the documentation for the ``MDP`` class for\n",
    "        details.  Default: 0.01.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations. If the value given is greater than a\n",
    "        computed bound, a warning informs that the computed bound will be used\n",
    "        instead. By default, if ``discount`` is not equal to 1, a bound for\n",
    "        ``max_iter`` is computed, otherwise ``max_iter`` = 1000. See the\n",
    "        documentation for the ``MDP`` class for further details.\n",
    "    initial_value : array, optional\n",
    "        The starting value function. Default: a vector of zeros.\n",
    "\n",
    "    Data Attributes\n",
    "    ---------------\n",
    "    V : tuple\n",
    "        The optimal value function.\n",
    "    policy : tuple\n",
    "        The optimal policy function. Each element is an integer corresponding\n",
    "        to an action which maximises the value function in that state.\n",
    "    iter : int\n",
    "        The number of iterations taken to complete the computation.\n",
    "    time : float\n",
    "        The amount of CPU time used to run the algorithm.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run()\n",
    "        Do the algorithm iteration.\n",
    "   \n",
    "### MDP: Value Iteration Run Method\n",
    "\n",
    "Once initialized, the run method calculates the optimal values and optimal policy at each state using the Bellman operator until convergence \n",
    "which is measured by epsilon.  \n",
    "\n",
    "        def run(self):\n",
    "        # Run the value iteration algorithm.\n",
    "\n",
    "        if self.verbose:\n",
    "            print('  Iteration\\t\\tV-variation')\n",
    "\n",
    "        self.time = _time.time()\n",
    "        while True:\n",
    "            self.iter += 1\n",
    "\n",
    "            Vprev = self.V.copy()\n",
    "\n",
    "            # Bellman Operator: compute policy and value functions\n",
    "            self.policy, self.V = self._bellmanOperator()\n",
    "\n",
    "            # The values, based on Q. For the function \"max()\": the option\n",
    "            # \"axis\" means the axis along which to operate. In this case it\n",
    "            # finds the maximum of the the rows. (Operates along the columns?)\n",
    "            variation = _util.getSpan(self.V - Vprev)\n",
    "\n",
    "            if self.verbose:\n",
    "                print((\"    %s\\t\\t  %s\" % (self.iter, variation)))\n",
    "\n",
    "            if variation < self.thresh:\n",
    "                if self.verbose:\n",
    "                    print(_MSG_STOP_EPSILON_OPTIMAL_POLICY)\n",
    "                break\n",
    "            elif self.iter == self.max_iter:\n",
    "                if self.verbose:\n",
    "                    print(_MSG_STOP_MAX_ITER)\n",
    "                break\n",
    "\n",
    "        # store value and policy as tuples\n",
    "        self.V = tuple(self.V.tolist())\n",
    "        self.policy = tuple(self.policy.tolist())\n",
    "\n",
    "        self.time = _time.time() - self.time\n",
    "\n",
    "The bellman operator is as follows:\n",
    "\n",
    "        def _bellmanOperator(self, V=None):\n",
    "            # Apply the Bellman operator on the value function.\n",
    "            #\n",
    "            # Updates the value function and the Vprev-improving policy.\n",
    "            #\n",
    "            # Returns: (policy, value), tuple of new policy and its value\n",
    "            #\n",
    "            # If V hasn't been sent into the method, then we assume to be working\n",
    "            # on the objects V attribute\n",
    "            if V is None:\n",
    "                # this V should be a reference to the data rather than a copy\n",
    "                V = self.V\n",
    "            else:\n",
    "                # make sure the user supplied V is of the right shape\n",
    "                try:\n",
    "                    assert V.shape in ((self.S,), (1, self.S)), \"V is not the \" \\\n",
    "                        \"right shape (Bellman operator).\"\n",
    "                except AttributeError:\n",
    "                    raise TypeError(\"V must be a numpy array or matrix.\")\n",
    "            # Looping through each action the the Q-value matrix is calculated.\n",
    "            # P and V can be any object that supports indexing, so it is important\n",
    "            # that you know they define a valid MDP before calling the\n",
    "            # _bellmanOperator method. Otherwise the results will be meaningless.\n",
    "            Q = _np.empty((self.A, self.S))\n",
    "            for aa in range(self.A):\n",
    "                Q[aa] = self.R[aa] + self.discount * self.P[aa].dot(V)\n",
    "    \n",
    "            # Get the policy and value, for now it is being returned but...\n",
    "            # Which way is better?\n",
    "            # 1. Return, (policy, value)\n",
    "    \n",
    "            Qmax = (Q.argmax(axis=0), Q.max(axis=0))\n",
    "            return Qmax\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
